======
MLflow
======

`MLflow <https://mlflow.org/>`_ is an open source framework for tracking ML experiments,
packaging ML code for training pipelines, and capturing models logged from experiments.
It enables data scientists to iterate quickly during model development while keeping
their experiments and training pipelines reproducible.

VtsServing, on the other hand, focuses on ML in production. By design, VtsServing is agnostic
to the experimentation platform and the model development environment.

Comparing to the MLflow model registry, VtsServing's model format and model store is
designed for managing model artifacts that will be used for building, testing, and
deploying prediction services. It is best fitted to manage your ‚Äúfinalized model‚Äù, sets
of models that yield the best outcomes from your periodic training pipelines and are
meant for running in production.

VtsServing integrates with MLflow natively. Users can not only port over models logged with
MLflow Tracking to VtsServing for high-performance model serving but also combine MLFlow
projects and pipelines with VtsServing's model deployment workflow in an efficient manner.


Compatibility
-------------

VtsServing supports MLflow 0.9 and above.

Examples
--------

Besides this documentation, also check out code samples demonstrating VtsServing and MLflow
integration at: `vtsserving/examples: MLflow Examples <https://github.com/vtsserving/VtsServing/tree/main/examples/mlflow>`_.


Import an MLflow model
----------------------

`MLflow Model <https://www.mlflow.org/docs/latest/models.html>`_ is a format for saving
trained model artifacts in MLflow experiments and pipelines. VtsServing supports importing
MLflow model to its own format for model serving. For example:

.. code-block:: python

    mlflow.sklearn.save_model(model, "./my_model")
    vtsserving.mlflow.import_model("my_sklearn_model", model_uri="./my_model")


.. code-block:: python

    with mlflow.start_run():
        mlflow.pytorch.log_model(model, artifact_path="pytorch-model")

        model_uri = mlflow.get_artifact_uri("pytorch-model")
        vts_model = vtsserving.mlflow.import_model(
            'mlflow_pytorch_mnist',
            model_uri,
            signatures={'predict': {'batchable': True}}
        )


The ``vtsserving.mlflow.import_model`` API is similar to the other ``save_model`` APIs
found in VtsServing, where the first argument represent the model name in VtsServing model
store. A new version will be automatically generated when a new MLflow model is
imported. Users can manage imported MLflow models same as models saved with other ML
frameworks:

.. code-block:: bash

    vtsserving models list mlflow_pytorch_mnist


The second argument ``model_uri`` takes a URI to the MLflow model. It can be a local
path, a ``'runs:/'`` URI, or a remote storage URI (e.g., an ``'s3://'`` URI). Here are
some example ``model_uri`` values commonly used in MLflow:

.. code-block::

    /Users/me/path/to/local/model
    ../relative/path/to/local/model
    s3://my_bucket/path/to/model
    runs:/<mlflow_run_id>/run-relative/path/to/model
    models:/<model_name>/<model_version>
    models:/<model_name>/<stage>


Running Imported Model
----------------------

MLflow models imported to VtsServing can be loaded back for running inference in a various
of ways.

Loading original model flavor
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For evaluation and testing purpose, sometimes it's convenient to load the model in its
native form

.. code-block:: python

    vts_model = vtsserving.mlflow.get("mlflow_pytorch_mnist:latest")
    mlflow_model_path = vts_model.path_of(vtsserving.mlflow.MLFLOW_MODEL_FOLDER)

    loaded_pytorch_model = mlflow.pytorch.load_model(mlflow_model_path)
    loaded_pytorch_model.to(device)
    loaded_pytorch_model.eval()
    with torch.no_grad():
        input_tensor = torch.from_numpy(test_input_arr).to(device)
        predictions = loaded_pytorch_model(input_tensor)


Loading Pyfunc flavor
~~~~~~~~~~~~~~~~~~~~~

By default, ``vtsserving.mflow.load_model`` will load the imported MLflow model using the
`python_function flavor <https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html>`_
for best compatibility across all ML frameworks supported by MLflow.

.. code-block:: python

    pyfunc_model: mlflow.pyfunc.PyFuncModel = vtsserving.mlflow.load_model("mlflow_pytorch_mnist:latest")
    predictions = pyfunc_model.predict(test_input_arr)


Using Model Runner
~~~~~~~~~~~~~~~~~~

Imported MLflow models can be loaded as VtsServing Runner for best performance in building
prediction service with VtsServing. To test out the runner API:

.. code-block:: python

    runner = vtsserving.mlflow.get("mlflow_pytorch_mnist:latest").to_runner()
    runner.init_local()
    runner.predict.run(input_df)

Learn more about VtsServing Runner at :doc:`/concepts/runner`.

Runner created from an MLflow model supports the following input types. Note that for
some ML frameworks, only a subset of this list is supported.

.. code-block:: python

    MLflowRunnerInput = Union[pandas.DataFrame, np.ndarray, List[Any], Dict[str, Any]]
    MLflowRunnerOutput = Union[pandas.DataFrame, pandas.Series, np.ndarray, list]

.. note::

    To use adaptive batching with a MLflow Runner, make sure to set
    ``signatures={'predict': {'batchable': True}}`` when importing the model:

    .. code-block:: python

        vts_model = vtsserving.mlflow.import_model(
            'mlflow_pytorch_mnist',
            model_uri,
            signatures={'predict': {'batchable': True}}
        )


Optimizations
~~~~~~~~~~~~~

There are two major limitations of using MLflow Runner in VtsServing:

* Lack of support for GPU
* Lack of support for multiple inference method

A common optimization we recommend, is to save trained model instance directly with VtsServing,
instead of importing MLflow pyfunc model. This makes it possible to run GPU inference and expose 
multiple inference signatures.

1. Save model directly with vtsserving

.. code-block:: python

    mlflow.sklearn.log_model(clf, "model")
    vtsserving.sklearn.save_model("iris_clf", clf)

2. Load original flavor and save with VtsServing

.. code-block:: python

    loaded_model = mlflow.sklearn.load_model(model_uri)
    vtsserving.sklearn.save_model("iris_clf", loaded_model)

This way, it goes back to a typically VtsServing workflow, which allow users to use a
Runner specifically built for the target ML framework, with GPU support and multiple
signatures available.


Build Prediction Service
------------------------

Here's an example ``vtsserving.Service`` built with a MLflow model:

.. code-block:: python

    import vtsserving
    import mlflow
    import torch

    mnist_runner = vtsserving.mlflow.get('mlflow_pytorch_mnist:latest').to_runner()

    svc = vtsserving.Service('mlflow_pytorch_mnist', runners=[ mnist_runner ])

    input_spec = vtsserving.io.NumpyNdarray(
        dtype="float32",
        shape=[-1, 1, 28, 28],
        enforce_shape=True,
        enforce_dtype=True,
    )

    @svc.api(input=input_spec, output=vtsserving.io.NumpyNdarray())
    def predict(input_arr):
        return mnist_runner.predict.run(input_arr)

To try out the full example, visit `vtsserving/examples: MLflow Pytorch Example <https://github.com/vtsserving/VtsServing/tree/main/examples/mlflow/pytorch>`_.


MLflow ü§ù VtsServing Workflow
--------------------------

There are numerous ways you can integrate VtsServing with your MLflow workflow for model serving and deployment.

1. Find ``model_uri`` from a MLflow model instance returned from ``log_model``:

.. code-block:: python

    # https://github.com/vtsserving/VtsServing/tree/main/examples/mlflow/sklearn_logistic_regression
    logged_model = mlflow.sklearn.log_model(lr, "model")
    print("Model saved in run %s" % mlflow.active_run().info.run_uuid)

    # Import logged mlflow model to VtsServing model store for serving:
    vts_model = vtsserving.mlflow.import_model('logistic_regression_model', logged_model.model_uri)
    print("Model imported to VtsServing: %s" % vts_model)

2. Find model artifact path inside current ``mlflow.run`` scope:

.. code-block:: python

    # https://github.com/vtsserving/VtsServing/tree/main/examples/mlflow/pytorch
    with mlflow.start_run():
        ...
        mlflow.pytorch.log_model(model, artifact_path="pytorch-model")
        model_uri = mlflow.get_artifact_uri("pytorch-model")
        vts_model = vtsserving.mlflow.import_model('mlflow_pytorch_mnist', model_uri)

3. When using ``autolog``, find ``model_uri`` by last active ``run_id``:

.. code-block:: python

    import mlflow
    import vtsserving
    from sklearn.linear_model import LinearRegression

    # enable autologging
    mlflow.sklearn.autolog()

    # prepare training data
    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
    y = np.dot(X, np.array([1, 2])) + 3

    # train a model
    model = LinearRegression()
    model.fit(X, y)

    # import logged MLflow model to VtsServing
    run_id = mlflow.last_active_run().info.run_id
    artifact_path = "model"
    model_uri = f"runs:/{run_id}/{artifact_path}"
    vts_model = vtsserving.mlflow.import_model('logistic_regression_model', model_uri)
    print(f"Model imported to VtsServing: {vts_model}")



4. Import a registered model on MLflow server

When using a MLflow tracking server, users can also import
`registered models <https://www.mlflow.org/docs/latest/model-registry.html#registering-a-model>`_
directly to VtsServing for serving.

.. code-block:: python

    # Import from a version:
    model_name = "sk-learn-random-forest-reg-model"
    model_version = 1
    model_uri=f"models:/{model_name}/{model_version}"
    vtsserving.mlflow.import_model('my_mlflow_model', model_uri)

    # Import from a stage:
    model_name = "sk-learn-random-forest-reg-model"
    stage = 'Staging'
    model_uri=f"models:/{model_name}/{stage}"
    vtsserving.mlflow.import_model('my_mlflow_model', model_uri)


Additional Tips
---------------

Use MLflow model dependencies config
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Most MLflow models bundles dependency information that is required for running framework model. 
If no additional dependencies are required in the :obj:`~vtsserving.Service` definition code, users may
pass through dependency requirements from within MLflow model to VtsServing.

First, put the following in your ``vtsfile.yaml`` build file:

.. code-block:: yaml

    python:
        requirements_txt: $VTSSERVING_MLFLOW_MODEL_PATH/mlflow_model/requirements.txt
        lock_packages: False

Alternatively, one can also use MLflow model's generated conda environment file:

.. code-block:: yaml

    conda:
        environment_yml: $VTSSERVING_MLFLOW_MODEL_PATH/mlflow_model/conda.yaml

This allows VtsServing to dynamically find the given dependency file based on a user-defined
environment variable. In this case, the ``vtsserving get`` CLI returns the path to the target
MLflow model folder and expose it to ``vtsserving build`` via the environment variable
``VTSSERVING_MLFLOW_MODEL_PATH``:

.. code-block:: bash

    export VTSSERVING_MLFLOW_MODEL_PATH=$(vtsserving models get my_mlflow_model:latest -o path)
    vtsserving build


Attach model params, metrics, and tags
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MLflow model format encapsulates lots of context information regarding the training metrics
and parameters. The following code snippet demonstrates how to package metadata logged from a given MLflow model to the VtsServing model store.


.. code-block:: python

    run_id = '0e4425ecbf3e4672ba0c1741651bb47a'
    run = mlflow.get_run(run_id)
    model_uri = f"{run.info.artifact_uri}/model"
    vtsserving.mlflow.import_model(
        "my_mlflow_model",
        model_uri,
        labels=run.data.tags,
        metadata={
            "metrics": run.data.metrics,
            "params": run.data.params,
        }
    )
